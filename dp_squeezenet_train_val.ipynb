{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dp_squeezenet_train_val.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deathboydmi/DynamicChannelPruning/blob/master/dp_squeezenet_train_val.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvfzQjO6SuhM",
        "colab_type": "code",
        "outputId": "207a9377-74ca-4e40-9227-328b6257f4e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!python3 --version"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.6.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRrLMoXAgKpx",
        "colab_type": "code",
        "outputId": "ebda73ff-d924-448e-e1cb-7c1d1e6b16df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!pip3 install torch==1.1.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.1.0 in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.1.0) (1.16.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZkJuD_ZTKQm",
        "colab_type": "code",
        "outputId": "9cff0755-212e-4740-a1c7-0d7223a6a4b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "!pip3 uninstall torchvision -y\n",
        "!git clone https://github.com/pytorch/vision.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling torchvision-0.2.3a0+d534785:\n",
            "  Successfully uninstalled torchvision-0.2.3a0+d534785\n",
            "fatal: destination path 'vision' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zmbt9YjhhB2F",
        "colab_type": "code",
        "outputId": "c753cb60-5678-436b-afd5-c6a16b8d58e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2753
        }
      },
      "source": [
        "!ls\n",
        "%cd vision\n",
        "!python setup.py install\n",
        "%cd ../"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data  sample_data  vision\n",
            "/content/vision\n",
            "Building wheel torchvision-0.2.3a0+d534785\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing torchvision.egg-info/PKG-INFO\n",
            "writing dependency_links to torchvision.egg-info/dependency_links.txt\n",
            "writing requirements to torchvision.egg-info/requires.txt\n",
            "writing top-level names to torchvision.egg-info/top_level.txt\n",
            "reading manifest template 'MANIFEST.in'\n",
            "warning: no previously-included files matching '__pycache__' found under directory '*'\n",
            "warning: no previously-included files matching '*.py[co]' found under directory '*'\n",
            "writing manifest file 'torchvision.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "copying torchvision/version.py -> build/lib/torchvision\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/torchvision\n",
            "creating build/bdist.linux-x86_64/egg/torchvision/models\n",
            "copying build/lib/torchvision/models/mobilenet.py -> build/bdist.linux-x86_64/egg/torchvision/models\n",
            "copying build/lib/torchvision/models/densenet.py -> build/bdist.linux-x86_64/egg/torchvision/models\n",
            "copying build/lib/torchvision/models/squeezenet.py -> build/bdist.linux-x86_64/egg/torchvision/models\n",
            "copying build/lib/torchvision/models/__init__.py -> build/bdist.linux-x86_64/egg/torchvision/models\n",
            "copying build/lib/torchvision/models/resnet.py -> build/bdist.linux-x86_64/egg/torchvision/models\n",
            "copying build/lib/torchvision/models/vgg.py -> build/bdist.linux-x86_64/egg/torchvision/models\n",
            "copying build/lib/torchvision/models/alexnet.py -> build/bdist.linux-x86_64/egg/torchvision/models\n",
            "copying build/lib/torchvision/models/googlenet.py -> build/bdist.linux-x86_64/egg/torchvision/models\n",
            "copying build/lib/torchvision/models/inception.py -> build/bdist.linux-x86_64/egg/torchvision/models\n",
            "copying build/lib/torchvision/models/utils.py -> build/bdist.linux-x86_64/egg/torchvision/models\n",
            "copying build/lib/torchvision/models/shufflenetv2.py -> build/bdist.linux-x86_64/egg/torchvision/models\n",
            "copying build/lib/torchvision/__init__.py -> build/bdist.linux-x86_64/egg/torchvision\n",
            "copying build/lib/torchvision/version.py -> build/bdist.linux-x86_64/egg/torchvision\n",
            "copying build/lib/torchvision/utils.py -> build/bdist.linux-x86_64/egg/torchvision\n",
            "creating build/bdist.linux-x86_64/egg/torchvision/datasets\n",
            "copying build/lib/torchvision/datasets/cifar.py -> build/bdist.linux-x86_64/egg/torchvision/datasets\n",
            "copying build/lib/torchvision/datasets/semeion.py -> build/bdist.linux-x86_64/egg/torchvision/datasets\n",
            "copying build/lib/torchvision/datasets/imagenet.py -> build/bdist.linux-x86_64/egg/torchvision/datasets\n",
            "copying build/lib/torchvision/datasets/sbu.py -> build/bdist.linux-x86_64/egg/torchvision/datasets\n",
            "copying build/lib/torchvision/datasets/__init__.py -> build/bdist.linux-x86_64/egg/torchvision/datasets\n",
            "copying build/lib/torchvision/datasets/mnist.py -> build/bdist.linux-x86_64/egg/torchvision/datasets\n",
            "copying build/lib/torchvision/datasets/phototour.py -> build/bdist.linux-x86_64/egg/torchvision/datasets\n",
            "copying build/lib/torchvision/datasets/celeba.py -> build/bdist.linux-x86_64/egg/torchvision/datasets\n",
            "copying build/lib/torchvision/datasets/cityscapes.py -> build/bdist.linux-x86_64/egg/torchvision/datasets\n",
            "copying build/lib/torchvision/datasets/lsun.py -> build/bdist.linux-x86_64/egg/torchvision/datasets\n",
            "copying build/lib/torchvision/datasets/omniglot.py -> build/bdist.linux-x86_64/egg/torchvision/datasets\n",
            "copying build/lib/torchvision/datasets/caltech.py -> build/bdist.linux-x86_64/egg/torchvision/datasets\n",
            "copying build/lib/torchvision/datasets/sbd.py -> build/bdist.linux-x86_64/egg/torchvision/datasets\n",
            "copying build/lib/torchvision/datasets/stl10.py -> build/bdist.linux-x86_64/egg/torchvision/datasets\n",
            "copying build/lib/torchvision/datasets/utils.py -> build/bdist.linux-x86_64/egg/torchvision/datasets\n",
            "copying build/lib/torchvision/datasets/vision.py -> build/bdist.linux-x86_64/egg/torchvision/datasets\n",
            "copying build/lib/torchvision/datasets/voc.py -> build/bdist.linux-x86_64/egg/torchvision/datasets\n",
            "copying build/lib/torchvision/datasets/folder.py -> build/bdist.linux-x86_64/egg/torchvision/datasets\n",
            "copying build/lib/torchvision/datasets/flickr.py -> build/bdist.linux-x86_64/egg/torchvision/datasets\n",
            "copying build/lib/torchvision/datasets/fakedata.py -> build/bdist.linux-x86_64/egg/torchvision/datasets\n",
            "copying build/lib/torchvision/datasets/svhn.py -> build/bdist.linux-x86_64/egg/torchvision/datasets\n",
            "copying build/lib/torchvision/datasets/coco.py -> build/bdist.linux-x86_64/egg/torchvision/datasets\n",
            "creating build/bdist.linux-x86_64/egg/torchvision/transforms\n",
            "copying build/lib/torchvision/transforms/__init__.py -> build/bdist.linux-x86_64/egg/torchvision/transforms\n",
            "copying build/lib/torchvision/transforms/transforms.py -> build/bdist.linux-x86_64/egg/torchvision/transforms\n",
            "copying build/lib/torchvision/transforms/functional.py -> build/bdist.linux-x86_64/egg/torchvision/transforms\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/models/mobilenet.py to mobilenet.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/models/densenet.py to densenet.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/models/squeezenet.py to squeezenet.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/models/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/models/resnet.py to resnet.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/models/vgg.py to vgg.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/models/alexnet.py to alexnet.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/models/googlenet.py to googlenet.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/models/inception.py to inception.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/models/utils.py to utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/models/shufflenetv2.py to shufflenetv2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/version.py to version.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/utils.py to utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/datasets/cifar.py to cifar.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/datasets/semeion.py to semeion.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/datasets/imagenet.py to imagenet.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/datasets/sbu.py to sbu.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/datasets/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/datasets/mnist.py to mnist.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/datasets/phototour.py to phototour.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/datasets/celeba.py to celeba.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/datasets/cityscapes.py to cityscapes.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/datasets/lsun.py to lsun.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/datasets/omniglot.py to omniglot.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/datasets/caltech.py to caltech.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/datasets/sbd.py to sbd.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/datasets/stl10.py to stl10.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/datasets/utils.py to utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/datasets/vision.py to vision.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/datasets/voc.py to voc.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/datasets/folder.py to folder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/datasets/flickr.py to flickr.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/datasets/fakedata.py to fakedata.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/datasets/svhn.py to svhn.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/datasets/coco.py to coco.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/transforms/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/transforms/transforms.py to transforms.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torchvision/transforms/functional.py to functional.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying torchvision.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying torchvision.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying torchvision.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying torchvision.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying torchvision.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying torchvision.egg-info/zip-safe -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "creating 'dist/torchvision-0.2.3a0+d534785-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing torchvision-0.2.3a0+d534785-py3.6.egg\n",
            "Copying torchvision-0.2.3a0+d534785-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding torchvision 0.2.3a0+d534785 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/torchvision-0.2.3a0+d534785-py3.6.egg\n",
            "Processing dependencies for torchvision==0.2.3a0+d534785\n",
            "Searching for Pillow==4.3.0\n",
            "Best match: Pillow 4.3.0\n",
            "Adding Pillow 4.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for torch==1.1.0\n",
            "Best match: torch 1.1.0\n",
            "Adding torch 1.1.0 to easy-install.pth file\n",
            "Installing convert-caffe2-to-onnx script to /usr/local/bin\n",
            "Installing convert-onnx-to-caffe2 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for six==1.12.0\n",
            "Best match: six 1.12.0\n",
            "Adding six 1.12.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for numpy==1.16.3\n",
            "Best match: numpy 1.16.3\n",
            "Adding numpy 1.16.3 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.6 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for olefile==0.46\n",
            "Best match: olefile 0.46\n",
            "Adding olefile 0.46 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Finished processing dependencies for torchvision==0.2.3a0+d534785\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N837fclzvHcX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "6936aaff-cc6d-499f-ee7d-a81bfc78339e"
      },
      "source": [
        "%ll\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 12\n",
            "drwxr-xr-x  3 root 4096 May  5 11:41 \u001b[0m\u001b[01;34mdata\u001b[0m/\n",
            "drwxr-xr-x  1 root 4096 Apr 29 16:32 \u001b[01;34msample_data\u001b[0m/\n",
            "drwxr-xr-x 10 root 4096 May  5 06:44 \u001b[01;34mvision\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L64kadFYynyx",
        "colab_type": "code",
        "outputId": "65db2637-1d02-4dc1-b58d-69955d152d34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        }
      },
      "source": [
        "%ll\n",
        "%cd data\n",
        "!wget http://www.image-net.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_img_train.tar -c\n",
        "!wget http://www.image-net.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_img_val.tar -c\n",
        "!wget http://www.image-net.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_devkit_t12.tar.gz -c\n",
        "%cd .."
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 12\n",
            "drwxr-xr-x  3 root 4096 May  5 11:41 \u001b[0m\u001b[01;34mdata\u001b[0m/\n",
            "drwxr-xr-x  1 root 4096 Apr 29 16:32 \u001b[01;34msample_data\u001b[0m/\n",
            "drwxr-xr-x 10 root 4096 May  5 06:44 \u001b[01;34mvision\u001b[0m/\n",
            "/content/data\n",
            "--2019-05-05 12:34:26--  http://www.image-net.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_img_train.tar\n",
            "Resolving www.image-net.org (www.image-net.org)... 171.64.68.16\n",
            "Connecting to www.image-net.org (www.image-net.org)|171.64.68.16|:80... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n",
            "--2019-05-05 12:34:27--  http://www.image-net.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_img_val.tar\n",
            "Resolving www.image-net.org (www.image-net.org)... 171.64.68.16\n",
            "Connecting to www.image-net.org (www.image-net.org)|171.64.68.16|:80... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n",
            "--2019-05-05 12:34:29--  http://www.image-net.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_devkit_t12.tar.gz\n",
            "Resolving www.image-net.org (www.image-net.org)... 171.64.68.16\n",
            "Connecting to www.image-net.org (www.image-net.org)|171.64.68.16|:80... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s-qJAxUTSmB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as func\n",
        "import torch.nn.init as init\n",
        "\n",
        "\n",
        "class DynamicPruning(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_layer_channels=None):\n",
        "        super(DynamicPruning, self).__init__()\n",
        "\n",
        "        if hidden_layer_channels is None:\n",
        "            hidden_layer_channels = in_channels // 16\n",
        "            if hidden_layer_channels < 4:\n",
        "                hidden_layer_channels = 4\n",
        "\n",
        "        self.gavgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = nn.Conv2d(in_channels, hidden_layer_channels,\n",
        "                             kernel_size=1, stride=1)\n",
        "        self.fc2 = nn.Conv2d(hidden_layer_channels,\n",
        "                             in_channels, kernel_size=1, stride=1)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.constant_(self.fc1.bias, 0)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        nn.init.constant_(self.fc2.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.gavgpool(x)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = torch.clamp(x, 0, 1)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = torch.clamp(x, 0, 1)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class DP_Conv2d(nn.Module):\n",
        "    def __init__(self, in_channels=0, out_channels=0,\n",
        "                 kernel_size=0, stride=1, padding=0, dilation=1,\n",
        "                 groups=1, bias=True, padding_mode='zeros',\n",
        "                 conv2d=None, hidden_layer_channels=None):\n",
        "        super(DP_Conv2d, self).__init__()\n",
        "        if in_channels is 0 or out_channels is 0 or kernel_size is 0:\n",
        "            if conv2d is None:\n",
        "                assert()\n",
        "            else:\n",
        "                self.__init__from_Conv2d(conv2d, hidden_layer_channels)\n",
        "        else:\n",
        "            if hidden_layer_channels is None:\n",
        "                hidden_layer_channels = out_channels // 16\n",
        "                if hidden_layer_channels < 4:\n",
        "                    hidden_layer_channels = 4\n",
        "\n",
        "            self.prun = DynamicPruning(in_channels, hidden_layer_channels)\n",
        "            self.conv = nn.Conv2d(in_channels, out_channels,\n",
        "                                  kernel_size, stride=stride,\n",
        "                                  padding=padding, dilation=dilation,\n",
        "                                  groups=groups, bias=bias,\n",
        "                                  padding_mode=padding_mode)\n",
        "\n",
        "    def __init__from_Conv2d(self, conv2d, hidden_layer_channels=None):\n",
        "        if not isinstance(conv2d, nn.Conv2d):\n",
        "            assert()\n",
        "        if hidden_layer_channels is None:\n",
        "            hidden_layer_channels = conv2d.out_channels // 16\n",
        "            if hidden_layer_channels < 4:\n",
        "                hidden_layer_channels = 4\n",
        "\n",
        "        self.prun = DynamicPruning(\n",
        "            conv2d.in_channels, hidden_layer_channels)\n",
        "        self.conv = conv2d\n",
        "\n",
        "    def forward(self, x):\n",
        "        dp_res = self.prun(x)\n",
        "        x = x * dp_res\n",
        "        x = self.conv(x)\n",
        "\n",
        "        return x\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGd7BnetUKaH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1847
        },
        "outputId": "e37861d8-9f66-4872-e4c4-cfc82f901fd3"
      },
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "class FireDP(nn.Module):\n",
        "    def __init__(self, fire):\n",
        "        super(FireDP, self).__init__()\n",
        "\n",
        "        self.squeeze = fire.squeeze\n",
        "        self.squeeze_activation = fire.squeeze_activation\n",
        "\n",
        "        self.expand1x1 = fire.expand1x1\n",
        "        self.expand1x1_activation = fire.expand1x1_activation\n",
        "\n",
        "        self.prun = DynamicPruning(self.squeeze.out_channels)\n",
        "\n",
        "        self.expand3x3 = fire.expand3x3\n",
        "        self.expand3x3_activation = fire.expand3x3_activation\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.squeeze_activation(self.squeeze(x))\n",
        "        dp_res = self.prun(x)\n",
        "        dp_res = dp_res * x\n",
        "\n",
        "        return torch.cat([\n",
        "            self.expand1x1_activation(self.expand1x1(x)),\n",
        "            self.expand3x3_activation(self.expand3x3(dp_res))\n",
        "        ], 1)\n",
        "\n",
        "\n",
        "class SqueezeNet_DP(nn.Module):\n",
        "    def __init__(self, version=1.0, num_classes=1000):\n",
        "        super(SqueezeNet_DP, self).__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        pretrain_model = models.squeezenet1_1(\n",
        "            pretrained=True, num_classes=num_classes)\n",
        "\n",
        "        self.features = pretrain_model.features\n",
        "        self.features[3] = FireDP(fire=self.features[3])\n",
        "        self.features[4] = FireDP(fire=self.features[4])\n",
        "        self.features[6] = FireDP(fire=self.features[6])\n",
        "        self.features[7] = FireDP(fire=self.features[7])\n",
        "\n",
        "        self.classifier = pretrain_model.classifier\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x.view(x.size(0), self.num_classes)\n",
        "\n",
        "print(SqueezeNet_DP())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SqueezeNet_DP(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
            "    (1): ReLU(inplace)\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (3): FireDP(\n",
            "      (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace)\n",
            "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace)\n",
            "      (prun): DynamicPruning(\n",
            "        (gavgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (fc2): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace)\n",
            "    )\n",
            "    (4): FireDP(\n",
            "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace)\n",
            "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace)\n",
            "      (prun): DynamicPruning(\n",
            "        (gavgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (fc2): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace)\n",
            "    )\n",
            "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (6): FireDP(\n",
            "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace)\n",
            "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace)\n",
            "      (prun): DynamicPruning(\n",
            "        (gavgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (fc2): Conv2d(4, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace)\n",
            "    )\n",
            "    (7): FireDP(\n",
            "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace)\n",
            "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace)\n",
            "      (prun): DynamicPruning(\n",
            "        (gavgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc1): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (fc2): Conv2d(4, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace)\n",
            "    )\n",
            "    (8): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (9): Fire(\n",
            "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace)\n",
            "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace)\n",
            "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace)\n",
            "    )\n",
            "    (10): Fire(\n",
            "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace)\n",
            "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace)\n",
            "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace)\n",
            "    )\n",
            "    (11): Fire(\n",
            "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace)\n",
            "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace)\n",
            "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace)\n",
            "    )\n",
            "    (12): Fire(\n",
            "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace)\n",
            "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace)\n",
            "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.5)\n",
            "    (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (2): ReLU(inplace)\n",
            "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4bzOuwuUcZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import shutil\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.distributed as dist\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torch.utils.data.distributed\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "best_prec1 = 0\n",
        "best_total_num_zeros = 0\n",
        "\n",
        "DP_zeros = 0\n",
        "DP_loss = 0\n",
        "\n",
        "DP_num_zeros_for_each_block = [0, 0, 0, 0]\n",
        "\n",
        "learning_rate = 0.04\n",
        "batch_size = 128\n",
        "epochs = 70\n",
        "momentum = 0.9\n",
        "weight_decay = 0.0002\n",
        "resume = False\n",
        "start_epoch = 0\n",
        "evaluate = False\n",
        "print_freq = 10\n",
        "\n",
        "\n",
        "def main():\n",
        "    global best_prec1, best_total_num_zeros, DP_num_zeros_for_each_block\n",
        "    global learning_rate, batch_size, epochs, momentum, weight_decay\n",
        "    global start_epoch, resume, evaluate\n",
        "\n",
        "    random.seed(271728)\n",
        "    torch.manual_seed(271728)\n",
        "    cudnn.deterministic = True\n",
        "\n",
        "    model = SqueezeNet_DP()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss().cuda()\n",
        "    optimizer = torch.optim.SGD(filter(\n",
        "        lambda p: p.requires_grad, model.parameters()),\n",
        "        learning_rate,\n",
        "        momentum=momentum,\n",
        "        weight_decay=weight_decay)\n",
        "\n",
        "    if resume:\n",
        "        if os.path.isfile(resume):\n",
        "            print(\"=> loading checkpoint '{}'\".format(resume))\n",
        "            checkpoint = torch.load(resume)\n",
        "            start_epoch = checkpoint['epoch']\n",
        "            best_prec1 = checkpoint['best_prec1']\n",
        "            model.load_state_dict(checkpoint['state_dict'])\n",
        "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
        "                  .format(resume, checkpoint['epoch']))\n",
        "        else:\n",
        "            print(\"=> no checkpoint found at '{}'\".format(resume))\n",
        "\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "\n",
        "    train_dataset = datasets.ImageNet('./data/', split='train', download=True,\n",
        "                                      transform=transforms.Compose([\n",
        "                                          transforms.RandomResizedCrop(224),\n",
        "                                          transforms.RandomHorizontalFlip(),\n",
        "                                          transforms.ToTensor(),\n",
        "                                          normalize\n",
        "                                      ]))\n",
        "\n",
        "    train_sampler = None\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=batch_size,\n",
        "        shuffle=(train_sampler is None),\n",
        "        num_workers=8, pin_memory=True, sampler=train_sampler)\n",
        "\n",
        "    val_dataset = datasets.ImageNet('./data/', split='val', download=True, transform=transforms.Compose(\n",
        "        [transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), normalize]))\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "                                             batch_size=batch_size, shuffle=False,\n",
        "                                             num_workers=8, pin_memory=True)\n",
        "\n",
        "    if evaluate:\n",
        "        validate(val_loader, model, criterion)\n",
        "        return\n",
        "\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        train(train_loader, model, criterion, optimizer, epoch)\n",
        "        adjust_learning_rate(optimizer, epoch)\n",
        "\n",
        "        prec1 = validate(val_loader, model, criterion)\n",
        "\n",
        "        is_best_acc = prec1 > best_prec1\n",
        "\n",
        "        DP_total_num_zeros = 0\n",
        "        for num_zeros in DP_num_zeros_for_each_block:\n",
        "            DP_total_num_zeros += num_zeros\n",
        "        DP_num_zeros_for_each_block = [0, 0, 0, 0]\n",
        "\n",
        "        is_best_prun = DP_total_num_zeros > best_total_num_zeros\n",
        "\n",
        "        best_prec1 = max(prec1, best_prec1)\n",
        "        best_total_num_zeros = max(DP_total_num_zeros, best_total_num_zeros)\n",
        "        save_checkpoint({\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'best_prec1': best_prec1,\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "        }, is_best_acc, is_best_prun)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ywa17N8RaKfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    global DP_loss, DP_zeros\n",
        "    global print_freq\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "\n",
        "    handle_1 = model.features[3].prun.register_forward_hook(DP_results_train)\n",
        "    handle_2 = model.features[4].prun.register_forward_hook(DP_results_train)\n",
        "    handle_3 = model.features[6].prun.register_forward_hook(DP_results_train)\n",
        "    handle_4 = model.features[7].prun.register_forward_hook(DP_results_train)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        output = model(input)\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        print(loss, loss+DP_loss)\n",
        "        loss += DP_loss\n",
        "\n",
        "        losses.update(loss.item(), input.size(0))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % print_freq == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
        "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
        "                      data_time=data_time, loss=losses))\n",
        "\n",
        "        DP_loss = 0\n",
        "        DP_zeros = 0\n",
        "\n",
        "    handle_1.remove()\n",
        "    handle_2.remove()\n",
        "    handle_3.remove()\n",
        "    handle_4.remove()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS1uDDWHlnwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(val_loader, model, criterion):\n",
        "    global DP_zeros, DP_num_zeros_for_each_block, DP_loss\n",
        "    global print_freq\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "    dp_losses = AverageMeter()\n",
        "    dp_zeros = AverageMeter()\n",
        "\n",
        "    handle1 = model.features[3].prun.register_forward_hook(DP_get_num_zeros_1)\n",
        "    handle2 = model.features[4].prun.register_forward_hook(DP_get_num_zeros_2)\n",
        "    handle3 = model.features[6].prun.register_forward_hook(DP_get_num_zeros_3)\n",
        "    handle4 = model.features[7].prun.register_forward_hook(DP_get_num_zeros_4)\n",
        "\n",
        "    handle_1 = model.features[3].prun.register_forward_hook(DP_results)\n",
        "    handle_2 = model.features[4].prun.register_forward_hook(DP_results)\n",
        "    handle_3 = model.features[6].prun.register_forward_hook(DP_results)\n",
        "    handle_4 = model.features[7].prun.register_forward_hook(DP_results)\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        end = time.time()\n",
        "        for i, (input, target) in enumerate(val_loader):\n",
        "\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target)\n",
        "            loss += DP_loss\n",
        "\n",
        "            prec1, prec5 = accuracy(output, target, topk=(1, 5))\n",
        "\n",
        "            losses.update(loss.item(), input.size(0))\n",
        "            dp_losses.update(DP_loss)\n",
        "            dp_zeros.update(DP_zeros)\n",
        "            top1.update(prec1[0], input.size(0))\n",
        "            top5.update(prec5[0], input.size(0))\n",
        "\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            print('Test: [{0}/{1}]\\t'\n",
        "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'DP_Loss {dp_loss.val:.4f} ({dp_loss.avg:.4f})\\t'\n",
        "                  'Number of zeros {num.val} ({num.avg:.3f})\\t'\n",
        "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
        "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
        "                      i, len(val_loader), batch_time=batch_time, loss=losses,\n",
        "                      dp_loss=dp_losses, num=dp_zeros,\n",
        "                      top1=top1, top5=top5))\n",
        "\n",
        "            DP_loss = 0\n",
        "            DP_zeros = 0\n",
        "\n",
        "            if i == len(val_loader) - 1:\n",
        "                last_bs = output.size(0)\n",
        "\n",
        "        handle1.remove()\n",
        "        handle2.remove()\n",
        "        handle3.remove()\n",
        "        handle4.remove()\n",
        "\n",
        "        handle_1.remove()\n",
        "        handle_2.remove()\n",
        "        handle_3.remove()\n",
        "        handle_4.remove()\n",
        "\n",
        "        DP_percentage_zeros_for_each_block = []\n",
        "        DP_percentage_zeros_for_each_block.append(\n",
        "            DP_num_zeros_for_each_block[0] / (\n",
        "                ((len(val_loader) - 1) * args.batch_size + last_bs) * 16\n",
        "            )\n",
        "        )\n",
        "        DP_percentage_zeros_for_each_block.append(\n",
        "            DP_num_zeros_for_each_block[1] / (\n",
        "                ((len(val_loader) - 1) * args.batch_size + last_bs) * 16\n",
        "            )\n",
        "        )\n",
        "        DP_percentage_zeros_for_each_block.append(\n",
        "            DP_num_zeros_for_each_block[2] / (\n",
        "                ((len(val_loader) - 1) * args.batch_size + last_bs) * 32\n",
        "            )\n",
        "        )\n",
        "        DP_percentage_zeros_for_each_block.append(\n",
        "            DP_num_zeros_for_each_block[3] / (\n",
        "                ((len(val_loader) - 1) * args.batch_size + last_bs) * 32\n",
        "            )\n",
        "        )\n",
        "\n",
        "        print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'\n",
        "              .format(top1=top1, top5=top5))\n",
        "        print('\\t', DP_percentage_zeros_for_each_block)\n",
        "\n",
        "    return top1.avg\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gi92y_nWmAgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def DP_results(self, input, output):\n",
        "    global DP_zeros, DP_loss\n",
        "    vec = output\n",
        "    vec = vec.cpu()\n",
        "\n",
        "    for i in vec.view(-1):\n",
        "        if i == 0:\n",
        "            DP_zeros += 1\n",
        "\n",
        "    DP_loss += torch.dist(vec.mean(), torch.tensor(0.5))\n",
        "\n",
        "\n",
        "def DP_results_train(self, input, output):\n",
        "    global DP_loss\n",
        "    vec = output\n",
        "    vec = vec.cpu()\n",
        "\n",
        "    DP_loss += torch.dist(vec.mean(), torch.tensor(0.5))\n",
        "\n",
        "\n",
        "def DP_get_num_zeros_1(self, input, output):\n",
        "    global DP_num_zeros_for_each_block\n",
        "    vec = output\n",
        "    vec = vec.cpu()\n",
        "    for i in vec.view(-1):\n",
        "        if i == 0:\n",
        "            DP_num_zeros_for_each_block[0] += 1\n",
        "\n",
        "\n",
        "def DP_get_num_zeros_2(self, input, output):\n",
        "    global DP_num_zeros_for_each_block\n",
        "    vec = output\n",
        "    vec = vec.cpu()\n",
        "    for i in vec.view(-1):\n",
        "        if i == 0:\n",
        "            DP_num_zeros_for_each_block[1] += 1\n",
        "\n",
        "\n",
        "def DP_get_num_zeros_3(self, input, output):\n",
        "    global DP_num_zeros_for_each_block\n",
        "    vec = output\n",
        "    vec = vec.cpu()\n",
        "    for i in vec.view(-1):\n",
        "        if i == 0:\n",
        "            DP_num_zeros_for_each_block[2] += 1\n",
        "\n",
        "\n",
        "def DP_get_num_zeros_4(self, input, output):\n",
        "    global DP_num_zeros_for_each_block\n",
        "    vec = output\n",
        "    vec = vec.cpu()\n",
        "    for i in vec.view(-1):\n",
        "        if i == 0:\n",
        "            DP_num_zeros_for_each_block[3] += 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXKSnH5jmGE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_checkpoint(state, is_best_acc, is_best_prun,\n",
        "                    filename='../DP_SqueezeNet/checkpoint.pth.tar'):\n",
        "    torch.save(state, filename)\n",
        "    if is_best_acc:\n",
        "        shutil.copyfile(filename,\n",
        "                        '../DP_SqueezeNet/model_best_acc.pth.tar')\n",
        "    if is_best_prun:\n",
        "        shutil.copyfile(filename,\n",
        "                        '../DP_SqueezeNet/model_best_prun.pth.tar')\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 15 epochs\"\"\"\n",
        "    lr = args.lr * (0.1 ** (epoch // 15))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTb7mkm6mOCB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8151
        },
        "outputId": "432ace87-8af6-4403-b1f3-029800b7a42e"
      },
      "source": [
        "main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You set download=True, but a folder 'train' already exist in the root directory. If you want to re-download or re-extract the archive, delete the folder.\n",
            "tensor(8.1584, grad_fn=<NllLossBackward>) tensor(9.4732, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][0/8271]\tTime 33.961 (33.961)\tData 20.083 (20.083)\tLoss 9.4732 (9.4732)\t\n",
            "tensor(6.8892, grad_fn=<NllLossBackward>) tensor(8.2524, grad_fn=<AddBackward0>)\n",
            "tensor(6.9285, grad_fn=<NllLossBackward>) tensor(8.2385, grad_fn=<AddBackward0>)\n",
            "tensor(7.0190, grad_fn=<NllLossBackward>) tensor(8.2817, grad_fn=<AddBackward0>)\n",
            "tensor(7.1149, grad_fn=<NllLossBackward>) tensor(8.3144, grad_fn=<AddBackward0>)\n",
            "tensor(7.0072, grad_fn=<NllLossBackward>) tensor(8.1494, grad_fn=<AddBackward0>)\n",
            "tensor(6.9196, grad_fn=<NllLossBackward>) tensor(8.0046, grad_fn=<AddBackward0>)\n",
            "tensor(6.8987, grad_fn=<NllLossBackward>) tensor(7.9288, grad_fn=<AddBackward0>)\n",
            "tensor(6.9389, grad_fn=<NllLossBackward>) tensor(7.8850, grad_fn=<AddBackward0>)\n",
            "tensor(6.9375, grad_fn=<NllLossBackward>) tensor(7.8281, grad_fn=<AddBackward0>)\n",
            "tensor(6.9152, grad_fn=<NllLossBackward>) tensor(7.7429, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][10/8271]\tTime 10.666 (12.885)\tData 0.000 (1.826)\tLoss 7.7429 (8.1908)\t\n",
            "tensor(6.9049, grad_fn=<NllLossBackward>) tensor(7.6682, grad_fn=<AddBackward0>)\n",
            "tensor(6.9082, grad_fn=<NllLossBackward>) tensor(7.6078, grad_fn=<AddBackward0>)\n",
            "tensor(6.9067, grad_fn=<NllLossBackward>) tensor(7.5464, grad_fn=<AddBackward0>)\n",
            "tensor(6.9091, grad_fn=<NllLossBackward>) tensor(7.4891, grad_fn=<AddBackward0>)\n",
            "tensor(6.9086, grad_fn=<NllLossBackward>) tensor(7.4571, grad_fn=<AddBackward0>)\n",
            "tensor(6.9131, grad_fn=<NllLossBackward>) tensor(7.4304, grad_fn=<AddBackward0>)\n",
            "tensor(6.9098, grad_fn=<NllLossBackward>) tensor(7.3849, grad_fn=<AddBackward0>)\n",
            "tensor(6.9078, grad_fn=<NllLossBackward>) tensor(7.3363, grad_fn=<AddBackward0>)\n",
            "tensor(6.9056, grad_fn=<NllLossBackward>) tensor(7.2858, grad_fn=<AddBackward0>)\n",
            "tensor(6.9108, grad_fn=<NllLossBackward>) tensor(7.2812, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][20/8271]\tTime 10.798 (11.857)\tData 0.000 (0.957)\tLoss 7.2812 (7.8374)\t\n",
            "tensor(6.9061, grad_fn=<NllLossBackward>) tensor(7.2597, grad_fn=<AddBackward0>)\n",
            "tensor(6.9103, grad_fn=<NllLossBackward>) tensor(7.2472, grad_fn=<AddBackward0>)\n",
            "tensor(6.9042, grad_fn=<NllLossBackward>) tensor(7.2189, grad_fn=<AddBackward0>)\n",
            "tensor(6.9089, grad_fn=<NllLossBackward>) tensor(7.1973, grad_fn=<AddBackward0>)\n",
            "tensor(6.9077, grad_fn=<NllLossBackward>) tensor(7.1651, grad_fn=<AddBackward0>)\n",
            "tensor(6.9078, grad_fn=<NllLossBackward>) tensor(7.1326, grad_fn=<AddBackward0>)\n",
            "tensor(6.9089, grad_fn=<NllLossBackward>) tensor(7.1018, grad_fn=<AddBackward0>)\n",
            "tensor(6.9074, grad_fn=<NllLossBackward>) tensor(7.0696, grad_fn=<AddBackward0>)\n",
            "tensor(6.9097, grad_fn=<NllLossBackward>) tensor(7.0473, grad_fn=<AddBackward0>)\n",
            "tensor(6.9093, grad_fn=<NllLossBackward>) tensor(7.0324, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][30/8271]\tTime 11.058 (11.589)\tData 0.000 (0.648)\tLoss 7.0324 (7.6148)\t\n",
            "tensor(6.9072, grad_fn=<NllLossBackward>) tensor(7.0221, grad_fn=<AddBackward0>)\n",
            "tensor(6.9066, grad_fn=<NllLossBackward>) tensor(7.0095, grad_fn=<AddBackward0>)\n",
            "tensor(6.9063, grad_fn=<NllLossBackward>) tensor(6.9918, grad_fn=<AddBackward0>)\n",
            "tensor(6.9074, grad_fn=<NllLossBackward>) tensor(6.9765, grad_fn=<AddBackward0>)\n",
            "tensor(6.9092, grad_fn=<NllLossBackward>) tensor(6.9592, grad_fn=<AddBackward0>)\n",
            "tensor(6.9068, grad_fn=<NllLossBackward>) tensor(6.9338, grad_fn=<AddBackward0>)\n",
            "tensor(6.9070, grad_fn=<NllLossBackward>) tensor(6.9101, grad_fn=<AddBackward0>)\n",
            "tensor(6.9081, grad_fn=<NllLossBackward>) tensor(6.9300, grad_fn=<AddBackward0>)\n",
            "tensor(6.9072, grad_fn=<NllLossBackward>) tensor(6.9459, grad_fn=<AddBackward0>)\n",
            "tensor(6.9067, grad_fn=<NllLossBackward>) tensor(6.9550, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][40/8271]\tTime 10.332 (11.367)\tData 0.000 (0.490)\tLoss 6.9550 (7.4559)\t\n",
            "tensor(6.9051, grad_fn=<NllLossBackward>) tensor(6.9575, grad_fn=<AddBackward0>)\n",
            "tensor(6.9060, grad_fn=<NllLossBackward>) tensor(6.9562, grad_fn=<AddBackward0>)\n",
            "tensor(6.9053, grad_fn=<NllLossBackward>) tensor(6.9492, grad_fn=<AddBackward0>)\n",
            "tensor(6.9052, grad_fn=<NllLossBackward>) tensor(6.9376, grad_fn=<AddBackward0>)\n",
            "tensor(6.9054, grad_fn=<NllLossBackward>) tensor(6.9396, grad_fn=<AddBackward0>)\n",
            "tensor(6.9068, grad_fn=<NllLossBackward>) tensor(6.9431, grad_fn=<AddBackward0>)\n",
            "tensor(6.9065, grad_fn=<NllLossBackward>) tensor(6.9453, grad_fn=<AddBackward0>)\n",
            "tensor(6.9091, grad_fn=<NllLossBackward>) tensor(6.9442, grad_fn=<AddBackward0>)\n",
            "tensor(6.9073, grad_fn=<NllLossBackward>) tensor(6.9342, grad_fn=<AddBackward0>)\n",
            "tensor(6.9066, grad_fn=<NllLossBackward>) tensor(6.9216, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][50/8271]\tTime 10.210 (11.171)\tData 0.000 (0.394)\tLoss 6.9216 (7.3553)\t\n",
            "tensor(6.9036, grad_fn=<NllLossBackward>) tensor(6.9116, grad_fn=<AddBackward0>)\n",
            "tensor(6.9071, grad_fn=<NllLossBackward>) tensor(6.9202, grad_fn=<AddBackward0>)\n",
            "tensor(6.9075, grad_fn=<NllLossBackward>) tensor(6.9269, grad_fn=<AddBackward0>)\n",
            "tensor(6.9062, grad_fn=<NllLossBackward>) tensor(6.9252, grad_fn=<AddBackward0>)\n",
            "tensor(6.9045, grad_fn=<NllLossBackward>) tensor(6.9180, grad_fn=<AddBackward0>)\n",
            "tensor(6.9071, grad_fn=<NllLossBackward>) tensor(6.9261, grad_fn=<AddBackward0>)\n",
            "tensor(6.9075, grad_fn=<NllLossBackward>) tensor(6.9284, grad_fn=<AddBackward0>)\n",
            "tensor(6.9053, grad_fn=<NllLossBackward>) tensor(6.9225, grad_fn=<AddBackward0>)\n",
            "tensor(6.9069, grad_fn=<NllLossBackward>) tensor(6.9185, grad_fn=<AddBackward0>)\n",
            "tensor(6.9054, grad_fn=<NllLossBackward>) tensor(6.9100, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][60/8271]\tTime 11.243 (11.082)\tData 0.000 (0.329)\tLoss 6.9100 (7.2841)\t\n",
            "tensor(6.9045, grad_fn=<NllLossBackward>) tensor(6.9144, grad_fn=<AddBackward0>)\n",
            "tensor(6.9051, grad_fn=<NllLossBackward>) tensor(6.9207, grad_fn=<AddBackward0>)\n",
            "tensor(6.9052, grad_fn=<NllLossBackward>) tensor(6.9225, grad_fn=<AddBackward0>)\n",
            "tensor(6.9065, grad_fn=<NllLossBackward>) tensor(6.9195, grad_fn=<AddBackward0>)\n",
            "tensor(6.9075, grad_fn=<NllLossBackward>) tensor(6.9138, grad_fn=<AddBackward0>)\n",
            "tensor(6.9056, grad_fn=<NllLossBackward>) tensor(6.9105, grad_fn=<AddBackward0>)\n",
            "tensor(6.9053, grad_fn=<NllLossBackward>) tensor(6.9143, grad_fn=<AddBackward0>)\n",
            "tensor(6.9060, grad_fn=<NllLossBackward>) tensor(6.9172, grad_fn=<AddBackward0>)\n",
            "tensor(6.9061, grad_fn=<NllLossBackward>) tensor(6.9149, grad_fn=<AddBackward0>)\n",
            "tensor(6.9067, grad_fn=<NllLossBackward>) tensor(6.9098, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][70/8271]\tTime 10.336 (10.982)\tData 0.000 (0.283)\tLoss 6.9098 (7.2322)\t\n",
            "tensor(6.9038, grad_fn=<NllLossBackward>) tensor(6.9122, grad_fn=<AddBackward0>)\n",
            "tensor(6.9056, grad_fn=<NllLossBackward>) tensor(6.9171, grad_fn=<AddBackward0>)\n",
            "tensor(6.9050, grad_fn=<NllLossBackward>) tensor(6.9165, grad_fn=<AddBackward0>)\n",
            "tensor(6.9058, grad_fn=<NllLossBackward>) tensor(6.9156, grad_fn=<AddBackward0>)\n",
            "tensor(6.9052, grad_fn=<NllLossBackward>) tensor(6.9094, grad_fn=<AddBackward0>)\n",
            "tensor(6.9049, grad_fn=<NllLossBackward>) tensor(6.9101, grad_fn=<AddBackward0>)\n",
            "tensor(6.9053, grad_fn=<NllLossBackward>) tensor(6.9149, grad_fn=<AddBackward0>)\n",
            "tensor(6.9043, grad_fn=<NllLossBackward>) tensor(6.9135, grad_fn=<AddBackward0>)\n",
            "tensor(6.9040, grad_fn=<NllLossBackward>) tensor(6.9093, grad_fn=<AddBackward0>)\n",
            "tensor(6.9061, grad_fn=<NllLossBackward>) tensor(6.9072, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][80/8271]\tTime 10.266 (10.903)\tData 0.000 (0.248)\tLoss 6.9072 (7.1927)\t\n",
            "tensor(6.9047, grad_fn=<NllLossBackward>) tensor(6.9134, grad_fn=<AddBackward0>)\n",
            "tensor(6.9081, grad_fn=<NllLossBackward>) tensor(6.9209, grad_fn=<AddBackward0>)\n",
            "tensor(6.9057, grad_fn=<NllLossBackward>) tensor(6.9181, grad_fn=<AddBackward0>)\n",
            "tensor(6.9074, grad_fn=<NllLossBackward>) tensor(6.9173, grad_fn=<AddBackward0>)\n",
            "tensor(6.9060, grad_fn=<NllLossBackward>) tensor(6.9133, grad_fn=<AddBackward0>)\n",
            "tensor(6.9026, grad_fn=<NllLossBackward>) tensor(6.9084, grad_fn=<AddBackward0>)\n",
            "tensor(6.9052, grad_fn=<NllLossBackward>) tensor(6.9137, grad_fn=<AddBackward0>)\n",
            "tensor(6.9053, grad_fn=<NllLossBackward>) tensor(6.9147, grad_fn=<AddBackward0>)\n",
            "tensor(6.9047, grad_fn=<NllLossBackward>) tensor(6.9112, grad_fn=<AddBackward0>)\n",
            "tensor(6.9059, grad_fn=<NllLossBackward>) tensor(6.9078, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][90/8271]\tTime 10.463 (10.857)\tData 0.000 (0.221)\tLoss 6.9078 (7.1621)\t\n",
            "tensor(6.9051, grad_fn=<NllLossBackward>) tensor(6.9128, grad_fn=<AddBackward0>)\n",
            "tensor(6.9070, grad_fn=<NllLossBackward>) tensor(6.9188, grad_fn=<AddBackward0>)\n",
            "tensor(6.9052, grad_fn=<NllLossBackward>) tensor(6.9172, grad_fn=<AddBackward0>)\n",
            "tensor(6.9037, grad_fn=<NllLossBackward>) tensor(6.9141, grad_fn=<AddBackward0>)\n",
            "tensor(6.9071, grad_fn=<NllLossBackward>) tensor(6.9141, grad_fn=<AddBackward0>)\n",
            "tensor(6.9018, grad_fn=<NllLossBackward>) tensor(6.9063, grad_fn=<AddBackward0>)\n",
            "tensor(6.9047, grad_fn=<NllLossBackward>) tensor(6.9135, grad_fn=<AddBackward0>)\n",
            "tensor(6.9063, grad_fn=<NllLossBackward>) tensor(6.9164, grad_fn=<AddBackward0>)\n",
            "tensor(6.9082, grad_fn=<NllLossBackward>) tensor(6.9142, grad_fn=<AddBackward0>)\n",
            "tensor(6.9031, grad_fn=<NllLossBackward>) tensor(6.9063, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][100/8271]\tTime 10.382 (10.826)\tData 0.000 (0.199)\tLoss 6.9063 (7.1375)\t\n",
            "tensor(6.9053, grad_fn=<NllLossBackward>) tensor(6.9159, grad_fn=<AddBackward0>)\n",
            "tensor(6.9054, grad_fn=<NllLossBackward>) tensor(6.9182, grad_fn=<AddBackward0>)\n",
            "tensor(6.9037, grad_fn=<NllLossBackward>) tensor(6.9152, grad_fn=<AddBackward0>)\n",
            "tensor(6.9045, grad_fn=<NllLossBackward>) tensor(6.9148, grad_fn=<AddBackward0>)\n",
            "tensor(6.9039, grad_fn=<NllLossBackward>) tensor(6.9080, grad_fn=<AddBackward0>)\n",
            "tensor(6.9054, grad_fn=<NllLossBackward>) tensor(6.9120, grad_fn=<AddBackward0>)\n",
            "tensor(6.9062, grad_fn=<NllLossBackward>) tensor(6.9177, grad_fn=<AddBackward0>)\n",
            "tensor(6.8999, grad_fn=<NllLossBackward>) tensor(6.9106, grad_fn=<AddBackward0>)\n",
            "tensor(6.9070, grad_fn=<NllLossBackward>) tensor(6.9128, grad_fn=<AddBackward0>)\n",
            "tensor(6.9054, grad_fn=<NllLossBackward>) tensor(6.9074, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][110/8271]\tTime 10.921 (10.800)\tData 0.000 (0.181)\tLoss 6.9074 (7.1173)\t\n",
            "tensor(6.9045, grad_fn=<NllLossBackward>) tensor(6.9115, grad_fn=<AddBackward0>)\n",
            "tensor(6.9042, grad_fn=<NllLossBackward>) tensor(6.9164, grad_fn=<AddBackward0>)\n",
            "tensor(6.9068, grad_fn=<NllLossBackward>) tensor(6.9200, grad_fn=<AddBackward0>)\n",
            "tensor(6.9042, grad_fn=<NllLossBackward>) tensor(6.9142, grad_fn=<AddBackward0>)\n",
            "tensor(6.9035, grad_fn=<NllLossBackward>) tensor(6.9080, grad_fn=<AddBackward0>)\n",
            "tensor(6.9016, grad_fn=<NllLossBackward>) tensor(6.9062, grad_fn=<AddBackward0>)\n",
            "tensor(6.9042, grad_fn=<NllLossBackward>) tensor(6.9134, grad_fn=<AddBackward0>)\n",
            "tensor(6.9047, grad_fn=<NllLossBackward>) tensor(6.9158, grad_fn=<AddBackward0>)\n",
            "tensor(6.9095, grad_fn=<NllLossBackward>) tensor(6.9177, grad_fn=<AddBackward0>)\n",
            "tensor(6.9058, grad_fn=<NllLossBackward>) tensor(6.9089, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][120/8271]\tTime 10.396 (10.778)\tData 0.000 (0.166)\tLoss 6.9089 (7.1004)\t\n",
            "tensor(6.9008, grad_fn=<NllLossBackward>) tensor(6.9091, grad_fn=<AddBackward0>)\n",
            "tensor(6.9041, grad_fn=<NllLossBackward>) tensor(6.9165, grad_fn=<AddBackward0>)\n",
            "tensor(6.9018, grad_fn=<NllLossBackward>) tensor(6.9137, grad_fn=<AddBackward0>)\n",
            "tensor(6.9050, grad_fn=<NllLossBackward>) tensor(6.9158, grad_fn=<AddBackward0>)\n",
            "tensor(6.9020, grad_fn=<NllLossBackward>) tensor(6.9075, grad_fn=<AddBackward0>)\n",
            "tensor(6.9047, grad_fn=<NllLossBackward>) tensor(6.9115, grad_fn=<AddBackward0>)\n",
            "tensor(6.9046, grad_fn=<NllLossBackward>) tensor(6.9152, grad_fn=<AddBackward0>)\n",
            "tensor(6.9029, grad_fn=<NllLossBackward>) tensor(6.9130, grad_fn=<AddBackward0>)\n",
            "tensor(6.9063, grad_fn=<NllLossBackward>) tensor(6.9124, grad_fn=<AddBackward0>)\n",
            "tensor(6.9028, grad_fn=<NllLossBackward>) tensor(6.9083, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][130/8271]\tTime 10.389 (10.755)\tData 0.000 (0.154)\tLoss 6.9083 (7.0860)\t\n",
            "tensor(6.9042, grad_fn=<NllLossBackward>) tensor(6.9168, grad_fn=<AddBackward0>)\n",
            "tensor(6.9032, grad_fn=<NllLossBackward>) tensor(6.9190, grad_fn=<AddBackward0>)\n",
            "tensor(6.9058, grad_fn=<NllLossBackward>) tensor(6.9205, grad_fn=<AddBackward0>)\n",
            "tensor(6.9045, grad_fn=<NllLossBackward>) tensor(6.9149, grad_fn=<AddBackward0>)\n",
            "tensor(6.9031, grad_fn=<NllLossBackward>) tensor(6.9089, grad_fn=<AddBackward0>)\n",
            "tensor(6.9020, grad_fn=<NllLossBackward>) tensor(6.9106, grad_fn=<AddBackward0>)\n",
            "tensor(6.9064, grad_fn=<NllLossBackward>) tensor(6.9201, grad_fn=<AddBackward0>)\n",
            "tensor(6.9039, grad_fn=<NllLossBackward>) tensor(6.9156, grad_fn=<AddBackward0>)\n",
            "tensor(6.9040, grad_fn=<NllLossBackward>) tensor(6.9122, grad_fn=<AddBackward0>)\n",
            "tensor(6.9041, grad_fn=<NllLossBackward>) tensor(6.9074, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][140/8271]\tTime 10.420 (10.742)\tData 0.000 (0.143)\tLoss 6.9074 (7.0739)\t\n",
            "tensor(6.9044, grad_fn=<NllLossBackward>) tensor(6.9142, grad_fn=<AddBackward0>)\n",
            "tensor(6.9054, grad_fn=<NllLossBackward>) tensor(6.9194, grad_fn=<AddBackward0>)\n",
            "tensor(6.8999, grad_fn=<NllLossBackward>) tensor(6.9136, grad_fn=<AddBackward0>)\n",
            "tensor(6.9033, grad_fn=<NllLossBackward>) tensor(6.9120, grad_fn=<AddBackward0>)\n",
            "tensor(6.9065, grad_fn=<NllLossBackward>) tensor(6.9135, grad_fn=<AddBackward0>)\n",
            "tensor(6.9010, grad_fn=<NllLossBackward>) tensor(6.9110, grad_fn=<AddBackward0>)\n",
            "tensor(6.9044, grad_fn=<NllLossBackward>) tensor(6.9177, grad_fn=<AddBackward0>)\n",
            "tensor(6.9046, grad_fn=<NllLossBackward>) tensor(6.9182, grad_fn=<AddBackward0>)\n",
            "tensor(6.9040, grad_fn=<NllLossBackward>) tensor(6.9129, grad_fn=<AddBackward0>)\n",
            "tensor(6.9048, grad_fn=<NllLossBackward>) tensor(6.9080, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][150/8271]\tTime 10.333 (10.734)\tData 0.000 (0.133)\tLoss 6.9080 (7.0633)\t\n",
            "tensor(6.9016, grad_fn=<NllLossBackward>) tensor(6.9114, grad_fn=<AddBackward0>)\n",
            "tensor(6.9035, grad_fn=<NllLossBackward>) tensor(6.9175, grad_fn=<AddBackward0>)\n",
            "tensor(6.9025, grad_fn=<NllLossBackward>) tensor(6.9157, grad_fn=<AddBackward0>)\n",
            "tensor(6.9059, grad_fn=<NllLossBackward>) tensor(6.9145, grad_fn=<AddBackward0>)\n",
            "tensor(6.9055, grad_fn=<NllLossBackward>) tensor(6.9102, grad_fn=<AddBackward0>)\n",
            "tensor(6.9018, grad_fn=<NllLossBackward>) tensor(6.9071, grad_fn=<AddBackward0>)\n",
            "tensor(6.9014, grad_fn=<NllLossBackward>) tensor(6.9116, grad_fn=<AddBackward0>)\n",
            "tensor(6.9028, grad_fn=<NllLossBackward>) tensor(6.9150, grad_fn=<AddBackward0>)\n",
            "tensor(6.9042, grad_fn=<NllLossBackward>) tensor(6.9121, grad_fn=<AddBackward0>)\n",
            "tensor(6.9035, grad_fn=<NllLossBackward>) tensor(6.9068, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][160/8271]\tTime 11.165 (10.724)\tData 0.000 (0.125)\tLoss 6.9068 (7.0539)\t\n",
            "tensor(6.9051, grad_fn=<NllLossBackward>) tensor(6.9141, grad_fn=<AddBackward0>)\n",
            "tensor(6.9026, grad_fn=<NllLossBackward>) tensor(6.9134, grad_fn=<AddBackward0>)\n",
            "tensor(6.9041, grad_fn=<NllLossBackward>) tensor(6.9158, grad_fn=<AddBackward0>)\n",
            "tensor(6.9020, grad_fn=<NllLossBackward>) tensor(6.9117, grad_fn=<AddBackward0>)\n",
            "tensor(6.9041, grad_fn=<NllLossBackward>) tensor(6.9081, grad_fn=<AddBackward0>)\n",
            "tensor(6.9034, grad_fn=<NllLossBackward>) tensor(6.9100, grad_fn=<AddBackward0>)\n",
            "tensor(6.9041, grad_fn=<NllLossBackward>) tensor(6.9157, grad_fn=<AddBackward0>)\n",
            "tensor(6.9016, grad_fn=<NllLossBackward>) tensor(6.9125, grad_fn=<AddBackward0>)\n",
            "tensor(6.9048, grad_fn=<NllLossBackward>) tensor(6.9129, grad_fn=<AddBackward0>)\n",
            "tensor(6.9054, grad_fn=<NllLossBackward>) tensor(6.9121, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][170/8271]\tTime 10.559 (10.714)\tData 0.000 (0.118)\tLoss 6.9121 (7.0457)\t\n",
            "tensor(6.9077, grad_fn=<NllLossBackward>) tensor(6.9189, grad_fn=<AddBackward0>)\n",
            "tensor(6.9021, grad_fn=<NllLossBackward>) tensor(6.9157, grad_fn=<AddBackward0>)\n",
            "tensor(6.9030, grad_fn=<NllLossBackward>) tensor(6.9165, grad_fn=<AddBackward0>)\n",
            "tensor(6.9055, grad_fn=<NllLossBackward>) tensor(6.9133, grad_fn=<AddBackward0>)\n",
            "tensor(6.9049, grad_fn=<NllLossBackward>) tensor(6.9101, grad_fn=<AddBackward0>)\n",
            "tensor(6.9070, grad_fn=<NllLossBackward>) tensor(6.9165, grad_fn=<AddBackward0>)\n",
            "tensor(6.9023, grad_fn=<NllLossBackward>) tensor(6.9157, grad_fn=<AddBackward0>)\n",
            "tensor(6.9039, grad_fn=<NllLossBackward>) tensor(6.9167, grad_fn=<AddBackward0>)\n",
            "tensor(6.9018, grad_fn=<NllLossBackward>) tensor(6.9106, grad_fn=<AddBackward0>)\n",
            "tensor(6.9053, grad_fn=<NllLossBackward>) tensor(6.9101, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][180/8271]\tTime 10.446 (10.710)\tData 0.000 (0.111)\tLoss 6.9101 (7.0384)\t\n",
            "tensor(6.9064, grad_fn=<NllLossBackward>) tensor(6.9163, grad_fn=<AddBackward0>)\n",
            "tensor(6.9035, grad_fn=<NllLossBackward>) tensor(6.9163, grad_fn=<AddBackward0>)\n",
            "tensor(6.9054, grad_fn=<NllLossBackward>) tensor(6.9176, grad_fn=<AddBackward0>)\n",
            "tensor(6.9051, grad_fn=<NllLossBackward>) tensor(6.9141, grad_fn=<AddBackward0>)\n",
            "tensor(6.9027, grad_fn=<NllLossBackward>) tensor(6.9067, grad_fn=<AddBackward0>)\n",
            "tensor(6.9067, grad_fn=<NllLossBackward>) tensor(6.9127, grad_fn=<AddBackward0>)\n",
            "tensor(6.9019, grad_fn=<NllLossBackward>) tensor(6.9124, grad_fn=<AddBackward0>)\n",
            "tensor(6.9045, grad_fn=<NllLossBackward>) tensor(6.9155, grad_fn=<AddBackward0>)\n",
            "tensor(6.9019, grad_fn=<NllLossBackward>) tensor(6.9103, grad_fn=<AddBackward0>)\n",
            "tensor(6.9040, grad_fn=<NllLossBackward>) tensor(6.9093, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][190/8271]\tTime 11.467 (10.703)\tData 0.000 (0.105)\tLoss 6.9093 (7.0318)\t\n",
            "tensor(6.9012, grad_fn=<NllLossBackward>) tensor(6.9098, grad_fn=<AddBackward0>)\n",
            "tensor(6.9025, grad_fn=<NllLossBackward>) tensor(6.9130, grad_fn=<AddBackward0>)\n",
            "tensor(6.9057, grad_fn=<NllLossBackward>) tensor(6.9156, grad_fn=<AddBackward0>)\n",
            "tensor(6.9025, grad_fn=<NllLossBackward>) tensor(6.9100, grad_fn=<AddBackward0>)\n",
            "tensor(6.8984, grad_fn=<NllLossBackward>) tensor(6.9005, grad_fn=<AddBackward0>)\n",
            "tensor(6.9031, grad_fn=<NllLossBackward>) tensor(6.9109, grad_fn=<AddBackward0>)\n",
            "tensor(6.9052, grad_fn=<NllLossBackward>) tensor(6.9165, grad_fn=<AddBackward0>)\n",
            "tensor(6.9010, grad_fn=<NllLossBackward>) tensor(6.9140, grad_fn=<AddBackward0>)\n",
            "tensor(6.9060, grad_fn=<NllLossBackward>) tensor(6.9153, grad_fn=<AddBackward0>)\n",
            "tensor(6.9055, grad_fn=<NllLossBackward>) tensor(6.9115, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][200/8271]\tTime 10.389 (10.697)\tData 0.000 (0.100)\tLoss 6.9115 (7.0259)\t\n",
            "tensor(6.9058, grad_fn=<NllLossBackward>) tensor(6.9164, grad_fn=<AddBackward0>)\n",
            "tensor(6.9070, grad_fn=<NllLossBackward>) tensor(6.9208, grad_fn=<AddBackward0>)\n",
            "tensor(6.9058, grad_fn=<NllLossBackward>) tensor(6.9191, grad_fn=<AddBackward0>)\n",
            "tensor(6.9018, grad_fn=<NllLossBackward>) tensor(6.9095, grad_fn=<AddBackward0>)\n",
            "tensor(6.9065, grad_fn=<NllLossBackward>) tensor(6.9117, grad_fn=<AddBackward0>)\n",
            "tensor(6.9043, grad_fn=<NllLossBackward>) tensor(6.9150, grad_fn=<AddBackward0>)\n",
            "tensor(6.9013, grad_fn=<NllLossBackward>) tensor(6.9155, grad_fn=<AddBackward0>)\n",
            "tensor(6.9004, grad_fn=<NllLossBackward>) tensor(6.9142, grad_fn=<AddBackward0>)\n",
            "tensor(6.9041, grad_fn=<NllLossBackward>) tensor(6.9131, grad_fn=<AddBackward0>)\n",
            "tensor(6.9012, grad_fn=<NllLossBackward>) tensor(6.9064, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][210/8271]\tTime 10.425 (10.697)\tData 0.000 (0.095)\tLoss 6.9064 (7.0206)\t\n",
            "tensor(6.9022, grad_fn=<NllLossBackward>) tensor(6.9111, grad_fn=<AddBackward0>)\n",
            "tensor(6.9016, grad_fn=<NllLossBackward>) tensor(6.9135, grad_fn=<AddBackward0>)\n",
            "tensor(6.9043, grad_fn=<NllLossBackward>) tensor(6.9161, grad_fn=<AddBackward0>)\n",
            "tensor(6.9041, grad_fn=<NllLossBackward>) tensor(6.9125, grad_fn=<AddBackward0>)\n",
            "tensor(6.9039, grad_fn=<NllLossBackward>) tensor(6.9078, grad_fn=<AddBackward0>)\n",
            "tensor(6.9021, grad_fn=<NllLossBackward>) tensor(6.9084, grad_fn=<AddBackward0>)\n",
            "tensor(6.9008, grad_fn=<NllLossBackward>) tensor(6.9119, grad_fn=<AddBackward0>)\n",
            "tensor(6.9076, grad_fn=<NllLossBackward>) tensor(6.9187, grad_fn=<AddBackward0>)\n",
            "tensor(6.9012, grad_fn=<NllLossBackward>) tensor(6.9095, grad_fn=<AddBackward0>)\n",
            "tensor(6.9044, grad_fn=<NllLossBackward>) tensor(6.9089, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][220/8271]\tTime 11.195 (10.695)\tData 0.000 (0.091)\tLoss 6.9089 (7.0156)\t\n",
            "tensor(6.9014, grad_fn=<NllLossBackward>) tensor(6.9082, grad_fn=<AddBackward0>)\n",
            "tensor(6.9033, grad_fn=<NllLossBackward>) tensor(6.9135, grad_fn=<AddBackward0>)\n",
            "tensor(6.9033, grad_fn=<NllLossBackward>) tensor(6.9142, grad_fn=<AddBackward0>)\n",
            "tensor(6.9055, grad_fn=<NllLossBackward>) tensor(6.9132, grad_fn=<AddBackward0>)\n",
            "tensor(6.9027, grad_fn=<NllLossBackward>) tensor(6.9057, grad_fn=<AddBackward0>)\n",
            "tensor(6.9012, grad_fn=<NllLossBackward>) tensor(6.9111, grad_fn=<AddBackward0>)\n",
            "tensor(6.9008, grad_fn=<NllLossBackward>) tensor(6.9140, grad_fn=<AddBackward0>)\n",
            "tensor(6.9029, grad_fn=<NllLossBackward>) tensor(6.9141, grad_fn=<AddBackward0>)\n",
            "tensor(6.8997, grad_fn=<NllLossBackward>) tensor(6.9077, grad_fn=<AddBackward0>)\n",
            "tensor(6.9007, grad_fn=<NllLossBackward>) tensor(6.9049, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][230/8271]\tTime 11.413 (10.697)\tData 0.000 (0.087)\tLoss 6.9049 (7.0111)\t\n",
            "tensor(6.9025, grad_fn=<NllLossBackward>) tensor(6.9070, grad_fn=<AddBackward0>)\n",
            "tensor(6.9009, grad_fn=<NllLossBackward>) tensor(6.9105, grad_fn=<AddBackward0>)\n",
            "tensor(6.8982, grad_fn=<NllLossBackward>) tensor(6.9083, grad_fn=<AddBackward0>)\n",
            "tensor(6.9060, grad_fn=<NllLossBackward>) tensor(6.9126, grad_fn=<AddBackward0>)\n",
            "tensor(6.9060, grad_fn=<NllLossBackward>) tensor(6.9094, grad_fn=<AddBackward0>)\n",
            "tensor(6.8984, grad_fn=<NllLossBackward>) tensor(6.9066, grad_fn=<AddBackward0>)\n",
            "tensor(6.9057, grad_fn=<NllLossBackward>) tensor(6.9187, grad_fn=<AddBackward0>)\n",
            "tensor(6.9064, grad_fn=<NllLossBackward>) tensor(6.9188, grad_fn=<AddBackward0>)\n",
            "tensor(6.9024, grad_fn=<NllLossBackward>) tensor(6.9138, grad_fn=<AddBackward0>)\n",
            "tensor(6.9044, grad_fn=<NllLossBackward>) tensor(6.9115, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][240/8271]\tTime 10.423 (10.693)\tData 0.000 (0.084)\tLoss 6.9115 (7.0070)\t\n",
            "tensor(6.9008, grad_fn=<NllLossBackward>) tensor(6.9098, grad_fn=<AddBackward0>)\n",
            "tensor(6.9033, grad_fn=<NllLossBackward>) tensor(6.9141, grad_fn=<AddBackward0>)\n",
            "tensor(6.9059, grad_fn=<NllLossBackward>) tensor(6.9151, grad_fn=<AddBackward0>)\n",
            "tensor(6.8998, grad_fn=<NllLossBackward>) tensor(6.9095, grad_fn=<AddBackward0>)\n",
            "tensor(6.9027, grad_fn=<NllLossBackward>) tensor(6.9082, grad_fn=<AddBackward0>)\n",
            "tensor(6.9077, grad_fn=<NllLossBackward>) tensor(6.9204, grad_fn=<AddBackward0>)\n",
            "tensor(6.9031, grad_fn=<NllLossBackward>) tensor(6.9191, grad_fn=<AddBackward0>)\n",
            "tensor(6.9030, grad_fn=<NllLossBackward>) tensor(6.9175, grad_fn=<AddBackward0>)\n",
            "tensor(6.9023, grad_fn=<NllLossBackward>) tensor(6.9110, grad_fn=<AddBackward0>)\n",
            "tensor(6.9038, grad_fn=<NllLossBackward>) tensor(6.9097, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][250/8271]\tTime 11.063 (10.693)\tData 0.000 (0.080)\tLoss 6.9097 (7.0033)\t\n",
            "tensor(6.9057, grad_fn=<NllLossBackward>) tensor(6.9114, grad_fn=<AddBackward0>)\n",
            "tensor(6.9045, grad_fn=<NllLossBackward>) tensor(6.9143, grad_fn=<AddBackward0>)\n",
            "tensor(6.9046, grad_fn=<NllLossBackward>) tensor(6.9165, grad_fn=<AddBackward0>)\n",
            "tensor(6.9026, grad_fn=<NllLossBackward>) tensor(6.9120, grad_fn=<AddBackward0>)\n",
            "tensor(6.9031, grad_fn=<NllLossBackward>) tensor(6.9057, grad_fn=<AddBackward0>)\n",
            "tensor(6.9016, grad_fn=<NllLossBackward>) tensor(6.9089, grad_fn=<AddBackward0>)\n",
            "tensor(6.9034, grad_fn=<NllLossBackward>) tensor(6.9159, grad_fn=<AddBackward0>)\n",
            "tensor(6.9067, grad_fn=<NllLossBackward>) tensor(6.9207, grad_fn=<AddBackward0>)\n",
            "tensor(6.9040, grad_fn=<NllLossBackward>) tensor(6.9176, grad_fn=<AddBackward0>)\n",
            "tensor(6.9028, grad_fn=<NllLossBackward>) tensor(6.9107, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][260/8271]\tTime 10.727 (10.696)\tData 0.000 (0.077)\tLoss 6.9107 (6.9998)\t\n",
            "tensor(6.9051, grad_fn=<NllLossBackward>) tensor(6.9102, grad_fn=<AddBackward0>)\n",
            "tensor(6.9053, grad_fn=<NllLossBackward>) tensor(6.9139, grad_fn=<AddBackward0>)\n",
            "tensor(6.9038, grad_fn=<NllLossBackward>) tensor(6.9135, grad_fn=<AddBackward0>)\n",
            "tensor(6.9048, grad_fn=<NllLossBackward>) tensor(6.9116, grad_fn=<AddBackward0>)\n",
            "tensor(6.9005, grad_fn=<NllLossBackward>) tensor(6.9029, grad_fn=<AddBackward0>)\n",
            "tensor(6.9034, grad_fn=<NllLossBackward>) tensor(6.9114, grad_fn=<AddBackward0>)\n",
            "tensor(6.9027, grad_fn=<NllLossBackward>) tensor(6.9136, grad_fn=<AddBackward0>)\n",
            "tensor(6.9057, grad_fn=<NllLossBackward>) tensor(6.9186, grad_fn=<AddBackward0>)\n",
            "tensor(6.9031, grad_fn=<NllLossBackward>) tensor(6.9142, grad_fn=<AddBackward0>)\n",
            "tensor(6.9039, grad_fn=<NllLossBackward>) tensor(6.9102, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][270/8271]\tTime 10.479 (10.691)\tData 0.000 (0.074)\tLoss 6.9102 (6.9966)\t\n",
            "tensor(6.9028, grad_fn=<NllLossBackward>) tensor(6.9078, grad_fn=<AddBackward0>)\n",
            "tensor(6.9072, grad_fn=<NllLossBackward>) tensor(6.9150, grad_fn=<AddBackward0>)\n",
            "tensor(6.9025, grad_fn=<NllLossBackward>) tensor(6.9120, grad_fn=<AddBackward0>)\n",
            "tensor(6.9006, grad_fn=<NllLossBackward>) tensor(6.9090, grad_fn=<AddBackward0>)\n",
            "tensor(6.9067, grad_fn=<NllLossBackward>) tensor(6.9094, grad_fn=<AddBackward0>)\n",
            "tensor(6.9019, grad_fn=<NllLossBackward>) tensor(6.9098, grad_fn=<AddBackward0>)\n",
            "tensor(6.9033, grad_fn=<NllLossBackward>) tensor(6.9160, grad_fn=<AddBackward0>)\n",
            "tensor(6.8992, grad_fn=<NllLossBackward>) tensor(6.9130, grad_fn=<AddBackward0>)\n",
            "tensor(6.9099, grad_fn=<NllLossBackward>) tensor(6.9219, grad_fn=<AddBackward0>)\n",
            "tensor(6.9083, grad_fn=<NllLossBackward>) tensor(6.9139, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][280/8271]\tTime 10.558 (10.685)\tData 0.000 (0.072)\tLoss 6.9139 (6.9936)\t\n",
            "tensor(6.9040, grad_fn=<NllLossBackward>) tensor(6.9093, grad_fn=<AddBackward0>)\n",
            "tensor(6.9043, grad_fn=<NllLossBackward>) tensor(6.9141, grad_fn=<AddBackward0>)\n",
            "tensor(6.9054, grad_fn=<NllLossBackward>) tensor(6.9147, grad_fn=<AddBackward0>)\n",
            "tensor(6.9065, grad_fn=<NllLossBackward>) tensor(6.9129, grad_fn=<AddBackward0>)\n",
            "tensor(6.9082, grad_fn=<NllLossBackward>) tensor(6.9105, grad_fn=<AddBackward0>)\n",
            "tensor(6.9002, grad_fn=<NllLossBackward>) tensor(6.9086, grad_fn=<AddBackward0>)\n",
            "tensor(6.8963, grad_fn=<NllLossBackward>) tensor(6.9079, grad_fn=<AddBackward0>)\n",
            "tensor(6.9040, grad_fn=<NllLossBackward>) tensor(6.9174, grad_fn=<AddBackward0>)\n",
            "tensor(6.9031, grad_fn=<NllLossBackward>) tensor(6.9135, grad_fn=<AddBackward0>)\n",
            "tensor(6.9030, grad_fn=<NllLossBackward>) tensor(6.9085, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][290/8271]\tTime 10.266 (10.683)\tData 0.000 (0.069)\tLoss 6.9085 (6.9908)\t\n",
            "tensor(6.9072, grad_fn=<NllLossBackward>) tensor(6.9127, grad_fn=<AddBackward0>)\n",
            "tensor(6.9040, grad_fn=<NllLossBackward>) tensor(6.9128, grad_fn=<AddBackward0>)\n",
            "tensor(6.9021, grad_fn=<NllLossBackward>) tensor(6.9101, grad_fn=<AddBackward0>)\n",
            "tensor(6.9042, grad_fn=<NllLossBackward>) tensor(6.9106, grad_fn=<AddBackward0>)\n",
            "tensor(6.9077, grad_fn=<NllLossBackward>) tensor(6.9085, grad_fn=<AddBackward0>)\n",
            "tensor(6.9017, grad_fn=<NllLossBackward>) tensor(6.9060, grad_fn=<AddBackward0>)\n",
            "tensor(6.9051, grad_fn=<NllLossBackward>) tensor(6.9078, grad_fn=<AddBackward0>)\n",
            "tensor(6.9046, grad_fn=<NllLossBackward>) tensor(6.9110, grad_fn=<AddBackward0>)\n",
            "tensor(6.8951, grad_fn=<NllLossBackward>) tensor(6.9009, grad_fn=<AddBackward0>)\n",
            "tensor(6.9053, grad_fn=<NllLossBackward>) tensor(6.9083, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][300/8271]\tTime 10.292 (10.675)\tData 0.000 (0.067)\tLoss 6.9083 (6.9881)\t\n",
            "tensor(6.9046, grad_fn=<NllLossBackward>) tensor(6.9090, grad_fn=<AddBackward0>)\n",
            "tensor(6.9031, grad_fn=<NllLossBackward>) tensor(6.9099, grad_fn=<AddBackward0>)\n",
            "tensor(6.9039, grad_fn=<NllLossBackward>) tensor(6.9089, grad_fn=<AddBackward0>)\n",
            "tensor(6.9054, grad_fn=<NllLossBackward>) tensor(6.9087, grad_fn=<AddBackward0>)\n",
            "tensor(6.9063, grad_fn=<NllLossBackward>) tensor(6.9112, grad_fn=<AddBackward0>)\n",
            "tensor(6.9026, grad_fn=<NllLossBackward>) tensor(6.9062, grad_fn=<AddBackward0>)\n",
            "tensor(6.9046, grad_fn=<NllLossBackward>) tensor(6.9112, grad_fn=<AddBackward0>)\n",
            "tensor(6.9002, grad_fn=<NllLossBackward>) tensor(6.9068, grad_fn=<AddBackward0>)\n",
            "tensor(6.9032, grad_fn=<NllLossBackward>) tensor(6.9089, grad_fn=<AddBackward0>)\n",
            "tensor(6.9050, grad_fn=<NllLossBackward>) tensor(6.9098, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][310/8271]\tTime 10.335 (10.670)\tData 0.000 (0.065)\tLoss 6.9098 (6.9855)\t\n",
            "tensor(6.9042, grad_fn=<NllLossBackward>) tensor(6.9092, grad_fn=<AddBackward0>)\n",
            "tensor(6.8955, grad_fn=<NllLossBackward>) tensor(6.8970, grad_fn=<AddBackward0>)\n",
            "tensor(6.9054, grad_fn=<NllLossBackward>) tensor(6.9133, grad_fn=<AddBackward0>)\n",
            "tensor(6.9009, grad_fn=<NllLossBackward>) tensor(6.9108, grad_fn=<AddBackward0>)\n",
            "tensor(6.9052, grad_fn=<NllLossBackward>) tensor(6.9127, grad_fn=<AddBackward0>)\n",
            "tensor(6.9016, grad_fn=<NllLossBackward>) tensor(6.9050, grad_fn=<AddBackward0>)\n",
            "tensor(6.9020, grad_fn=<NllLossBackward>) tensor(6.9048, grad_fn=<AddBackward0>)\n",
            "tensor(6.9010, grad_fn=<NllLossBackward>) tensor(6.9045, grad_fn=<AddBackward0>)\n",
            "tensor(6.8994, grad_fn=<NllLossBackward>) tensor(6.9035, grad_fn=<AddBackward0>)\n",
            "tensor(6.9049, grad_fn=<NllLossBackward>) tensor(6.9070, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][320/8271]\tTime 10.437 (10.672)\tData 0.000 (0.063)\tLoss 6.9070 (6.9831)\t\n",
            "tensor(6.9066, grad_fn=<NllLossBackward>) tensor(6.9096, grad_fn=<AddBackward0>)\n",
            "tensor(6.9039, grad_fn=<NllLossBackward>) tensor(6.9095, grad_fn=<AddBackward0>)\n",
            "tensor(6.9042, grad_fn=<NllLossBackward>) tensor(6.9074, grad_fn=<AddBackward0>)\n",
            "tensor(6.9044, grad_fn=<NllLossBackward>) tensor(6.9114, grad_fn=<AddBackward0>)\n",
            "tensor(6.9038, grad_fn=<NllLossBackward>) tensor(6.9110, grad_fn=<AddBackward0>)\n",
            "tensor(6.8990, grad_fn=<NllLossBackward>) tensor(6.9044, grad_fn=<AddBackward0>)\n",
            "tensor(6.9029, grad_fn=<NllLossBackward>) tensor(6.9070, grad_fn=<AddBackward0>)\n",
            "tensor(6.9016, grad_fn=<NllLossBackward>) tensor(6.9064, grad_fn=<AddBackward0>)\n",
            "tensor(6.9022, grad_fn=<NllLossBackward>) tensor(6.9067, grad_fn=<AddBackward0>)\n",
            "tensor(6.9067, grad_fn=<NllLossBackward>) tensor(6.9134, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][330/8271]\tTime 10.416 (10.667)\tData 0.000 (0.061)\tLoss 6.9134 (6.9808)\t\n",
            "tensor(6.9035, grad_fn=<NllLossBackward>) tensor(6.9112, grad_fn=<AddBackward0>)\n",
            "tensor(6.9053, grad_fn=<NllLossBackward>) tensor(6.9093, grad_fn=<AddBackward0>)\n",
            "tensor(6.9042, grad_fn=<NllLossBackward>) tensor(6.9089, grad_fn=<AddBackward0>)\n",
            "tensor(6.9077, grad_fn=<NllLossBackward>) tensor(6.9151, grad_fn=<AddBackward0>)\n",
            "tensor(6.9012, grad_fn=<NllLossBackward>) tensor(6.9069, grad_fn=<AddBackward0>)\n",
            "tensor(6.9017, grad_fn=<NllLossBackward>) tensor(6.9032, grad_fn=<AddBackward0>)\n",
            "tensor(6.9051, grad_fn=<NllLossBackward>) tensor(6.9092, grad_fn=<AddBackward0>)\n",
            "tensor(6.9014, grad_fn=<NllLossBackward>) tensor(6.9047, grad_fn=<AddBackward0>)\n",
            "tensor(6.9048, grad_fn=<NllLossBackward>) tensor(6.9090, grad_fn=<AddBackward0>)\n",
            "tensor(6.9004, grad_fn=<NllLossBackward>) tensor(6.9029, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][340/8271]\tTime 10.819 (10.663)\tData 0.000 (0.059)\tLoss 6.9029 (6.9787)\t\n",
            "tensor(6.9088, grad_fn=<NllLossBackward>) tensor(6.9118, grad_fn=<AddBackward0>)\n",
            "tensor(6.9011, grad_fn=<NllLossBackward>) tensor(6.9065, grad_fn=<AddBackward0>)\n",
            "tensor(6.9056, grad_fn=<NllLossBackward>) tensor(6.9088, grad_fn=<AddBackward0>)\n",
            "tensor(6.9062, grad_fn=<NllLossBackward>) tensor(6.9112, grad_fn=<AddBackward0>)\n",
            "tensor(6.8966, grad_fn=<NllLossBackward>) tensor(6.9042, grad_fn=<AddBackward0>)\n",
            "tensor(6.9061, grad_fn=<NllLossBackward>) tensor(6.9119, grad_fn=<AddBackward0>)\n",
            "tensor(6.9038, grad_fn=<NllLossBackward>) tensor(6.9099, grad_fn=<AddBackward0>)\n",
            "tensor(6.9038, grad_fn=<NllLossBackward>) tensor(6.9104, grad_fn=<AddBackward0>)\n",
            "tensor(6.9008, grad_fn=<NllLossBackward>) tensor(6.9053, grad_fn=<AddBackward0>)\n",
            "tensor(6.9034, grad_fn=<NllLossBackward>) tensor(6.9084, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][350/8271]\tTime 10.687 (10.662)\tData 0.000 (0.057)\tLoss 6.9084 (6.9767)\t\n",
            "tensor(6.9021, grad_fn=<NllLossBackward>) tensor(6.9083, grad_fn=<AddBackward0>)\n",
            "tensor(6.9008, grad_fn=<NllLossBackward>) tensor(6.9032, grad_fn=<AddBackward0>)\n",
            "tensor(6.8985, grad_fn=<NllLossBackward>) tensor(6.9049, grad_fn=<AddBackward0>)\n",
            "tensor(6.9054, grad_fn=<NllLossBackward>) tensor(6.9143, grad_fn=<AddBackward0>)\n",
            "tensor(6.9056, grad_fn=<NllLossBackward>) tensor(6.9118, grad_fn=<AddBackward0>)\n",
            "tensor(6.9022, grad_fn=<NllLossBackward>) tensor(6.9052, grad_fn=<AddBackward0>)\n",
            "tensor(6.9042, grad_fn=<NllLossBackward>) tensor(6.9093, grad_fn=<AddBackward0>)\n",
            "tensor(6.8995, grad_fn=<NllLossBackward>) tensor(6.9037, grad_fn=<AddBackward0>)\n",
            "tensor(6.9042, grad_fn=<NllLossBackward>) tensor(6.9069, grad_fn=<AddBackward0>)\n",
            "tensor(6.9027, grad_fn=<NllLossBackward>) tensor(6.9058, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][360/8271]\tTime 10.361 (10.656)\tData 0.000 (0.056)\tLoss 6.9058 (6.9748)\t\n",
            "tensor(6.9011, grad_fn=<NllLossBackward>) tensor(6.9028, grad_fn=<AddBackward0>)\n",
            "tensor(6.9044, grad_fn=<NllLossBackward>) tensor(6.9107, grad_fn=<AddBackward0>)\n",
            "tensor(6.9066, grad_fn=<NllLossBackward>) tensor(6.9135, grad_fn=<AddBackward0>)\n",
            "tensor(6.9027, grad_fn=<NllLossBackward>) tensor(6.9059, grad_fn=<AddBackward0>)\n",
            "tensor(6.9081, grad_fn=<NllLossBackward>) tensor(6.9129, grad_fn=<AddBackward0>)\n",
            "tensor(6.8984, grad_fn=<NllLossBackward>) tensor(6.9050, grad_fn=<AddBackward0>)\n",
            "tensor(6.9022, grad_fn=<NllLossBackward>) tensor(6.9076, grad_fn=<AddBackward0>)\n",
            "tensor(6.9023, grad_fn=<NllLossBackward>) tensor(6.9049, grad_fn=<AddBackward0>)\n",
            "tensor(6.9016, grad_fn=<NllLossBackward>) tensor(6.9063, grad_fn=<AddBackward0>)\n",
            "tensor(6.9015, grad_fn=<NllLossBackward>) tensor(6.9057, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][370/8271]\tTime 10.204 (10.649)\tData 0.000 (0.054)\tLoss 6.9057 (6.9730)\t\n",
            "tensor(6.8995, grad_fn=<NllLossBackward>) tensor(6.9067, grad_fn=<AddBackward0>)\n",
            "tensor(6.9068, grad_fn=<NllLossBackward>) tensor(6.9140, grad_fn=<AddBackward0>)\n",
            "tensor(6.9009, grad_fn=<NllLossBackward>) tensor(6.9085, grad_fn=<AddBackward0>)\n",
            "tensor(6.9014, grad_fn=<NllLossBackward>) tensor(6.9050, grad_fn=<AddBackward0>)\n",
            "tensor(6.9018, grad_fn=<NllLossBackward>) tensor(6.9063, grad_fn=<AddBackward0>)\n",
            "tensor(6.9005, grad_fn=<NllLossBackward>) tensor(6.9037, grad_fn=<AddBackward0>)\n",
            "tensor(6.9023, grad_fn=<NllLossBackward>) tensor(6.9038, grad_fn=<AddBackward0>)\n",
            "tensor(6.9059, grad_fn=<NllLossBackward>) tensor(6.9111, grad_fn=<AddBackward0>)\n",
            "tensor(6.9020, grad_fn=<NllLossBackward>) tensor(6.9064, grad_fn=<AddBackward0>)\n",
            "tensor(6.9015, grad_fn=<NllLossBackward>) tensor(6.9068, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][380/8271]\tTime 11.008 (10.646)\tData 0.000 (0.053)\tLoss 6.9068 (6.9712)\t\n",
            "tensor(6.9016, grad_fn=<NllLossBackward>) tensor(6.9079, grad_fn=<AddBackward0>)\n",
            "tensor(6.8952, grad_fn=<NllLossBackward>) tensor(6.9008, grad_fn=<AddBackward0>)\n",
            "tensor(6.9040, grad_fn=<NllLossBackward>) tensor(6.9095, grad_fn=<AddBackward0>)\n",
            "tensor(6.9031, grad_fn=<NllLossBackward>) tensor(6.9093, grad_fn=<AddBackward0>)\n",
            "tensor(6.9017, grad_fn=<NllLossBackward>) tensor(6.9060, grad_fn=<AddBackward0>)\n",
            "tensor(6.9061, grad_fn=<NllLossBackward>) tensor(6.9097, grad_fn=<AddBackward0>)\n",
            "tensor(6.8991, grad_fn=<NllLossBackward>) tensor(6.9044, grad_fn=<AddBackward0>)\n",
            "tensor(6.8956, grad_fn=<NllLossBackward>) tensor(6.8982, grad_fn=<AddBackward0>)\n",
            "tensor(6.9033, grad_fn=<NllLossBackward>) tensor(6.9071, grad_fn=<AddBackward0>)\n",
            "tensor(6.9017, grad_fn=<NllLossBackward>) tensor(6.9068, grad_fn=<AddBackward0>)\n",
            "Epoch: [0][390/8271]\tTime 10.004 (10.632)\tData 0.000 (0.052)\tLoss 6.9068 (6.9696)\t\n",
            "tensor(6.9038, grad_fn=<NllLossBackward>) tensor(6.9086, grad_fn=<AddBackward0>)\n",
            "tensor(6.9085, grad_fn=<NllLossBackward>) tensor(6.9125, grad_fn=<AddBackward0>)\n",
            "tensor(6.9061, grad_fn=<NllLossBackward>) tensor(6.9083, grad_fn=<AddBackward0>)\n",
            "tensor(6.8960, grad_fn=<NllLossBackward>) tensor(6.9020, grad_fn=<AddBackward0>)\n",
            "tensor(6.8978, grad_fn=<NllLossBackward>) tensor(6.9033, grad_fn=<AddBackward0>)\n",
            "tensor(6.9026, grad_fn=<NllLossBackward>) tensor(6.9058, grad_fn=<AddBackward0>)\n",
            "tensor(6.8983, grad_fn=<NllLossBackward>) tensor(6.9060, grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}